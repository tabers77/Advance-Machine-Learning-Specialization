{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Coursera Final Project"},{"metadata":{},"cell_type":"markdown","source":"## Main Goal:\n\nWe are asking you to predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills."},{"metadata":{},"cell_type":"markdown","source":"## Resources: \n\n- https://www.kaggle.com/c/competitive-data-science-predict-future-sales\n- https://www.kaggle.com/c/competitive-data-science-final-project\n- https://www.kaggle.com/alessandrosolbiati/using-xgboost-for-time-series-prediction-top-20 \n- https://www.kaggle.com/dimitreoliveira/model-stacking-feature-engineering-and-eda \n- https://www.coursera.org/learn/competitive-data-science/peer/QJDGf/final-project"},{"metadata":{},"cell_type":"markdown","source":"## Feature description: \n- ID: an Id that represents a (Shop, Item) tuple within the test set\n- shop_id: unique identifier of a shop\n- item_id: unique identifier of a product\n- item_category_id: unique identifier of item category\n- item_cnt_day: number of products sold. You are predicting a monthly amount of this measure TARGET VALUE !!!\n- item_price: current price of an item\n- date: date in format dd/mm/yyyy\n- date_block_num: a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n- item_name: name of item\n- shop_name: name of shop\n- item_category_name: name of item category"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"############\n#Libraries\n############\n\nimport numpy as np\nimport pandas as pd \nfrom itertools import product\nfrom tqdm import tqdm_notebook\n\nimport sklearn \nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.model_selection import GridSearchCV\nimport seaborn as sns\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the versions of the libraries\nfor p in [np, pd, sklearn, sns , xgb]:\n    print (p.__name__, p.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#############\n# Datasets \n#############\n\nitems = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv',parse_dates=True)#item info. item id and category id \nitem_categories = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv',parse_dates=True) # item_category_id\nshops = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv',parse_dates=True)# the name of the shops. shop_id\ntrain = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv',parse_dates=True)# training data \ntest = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv',parse_dates=True) # testing data \nsample_submission = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv',parse_dates=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1 EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add item information to train \n\ntrain = pd.merge(train, items, on='item_id', how='left')\ntrain = train.drop('item_name', axis=1)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of extra products on the train set compared to the train set  \nset(train['shop_id'].unique()) - set(test['shop_id'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many items and shop ids that we have on train we dont have on test?\n\ntrain['shop_id'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(test['shop_id'].unique()) - set(train['shop_id'].unique()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many uniques shops and items are in the data ?  \n\nprint(test['shop_id'].nunique()) # 42\nprint(train['shop_id'].nunique()) # 60\nprint(test['item_id'].nunique()) # 5100\nprint(train['item_id'].nunique()) # 21807","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target distribution \nsns.boxplot(x ='item_cnt_day' , data = train )\n# There are some outliers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generating new features from text "},{"metadata":{"trusted":true},"cell_type":"code","source":"# TEST \n# here we add sum , mean and count \ntrain_month = train.sort_values('date').groupby(['date_block_num', 'shop_id', 'item_id'], as_index = False).agg({'item_price':['sum', 'mean', 'count'], 'item_cnt_day':['sum', 'mean','count'], 'item_category_id':['sum', 'mean', 'count']})\ntrain_month.columns = ['date_block_num', 'shop_id', 'item_id', 'sum_item_price', 'mean_item_price', 'count_item_price', 'sum_item_cnt', 'mean_item_cnt', 'count_item_cnt', 'sum_item_sales', 'mean_item_sales', 'count_item_sales']\ntrain_month.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of products sold \ntrain_month.groupby('date_block_num')['item_id'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_month.groupby('date_block_num')['count_item_cnt'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# what is the len here ? \nlen(train_month)\n# 1609124 I decrease the number here since I group by","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TEST\n# Grid with all combinations \ntemp = []\nfor i in range(34):\n    for shop in train_month['shop_id'].unique(): # shop uinique ids \n        for item in train_month['item_id'].unique(): #item unique ids \n            temp.append([i, shop, item]) # append to temp \n    \ntemp = pd.DataFrame(temp, columns=['date_block_num','shop_id','item_id']) # put temp on a dataframe \n\ntrain_month = pd.merge(temp, train_month, on = ['date_block_num','shop_id','item_id'], how = 'left')\ntrain_month.fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"block0 = train_month[train_month['date_block_num']==0]\n\nblock32 = train_month[train_month['date_block_num']== 32]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"block0['item_id'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"block32['item_id'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TEST\n# create month and year\ntrain_month['year'] = train_month['date_block_num'].apply(lambda x: (x // 12 + 2013))\ntrain_month['month'] = train_month['date_block_num'].apply(lambda x: (x % 12))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_month)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_month.groupby('month')['count_item_cnt'].sum().plot(figsize=(12,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Text\n- Lowercase\n- Lemmatization\n- Stemming\n- Stopwords\n\nPipeline: \n- 1 Preprocessing (lowercase, stemming, lemmatization or remove stopwords )\n- 2 Ngrams can help to use local context \n- 3 Post processing using TFIdf\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TEST\n# 1 Create new features from text\n\n# extract features from the text - transform item categories: \n\n# Translate the item category name and find meta category, subtype\nsymbols=(u\"абвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ\", u\"abvgdeejzijklmnoprstufhzcss_y_euaABVGDEEJZIJKLMNOPRSTUFHZCSS_Y_EUA\")\nenglish = {ord(a):ord(b) for a, b in zip(*symbols)}\n\n# create 3 new rows from text \nitem_categories['items_english'] = item_categories['item_category_name'].apply(lambda x: x.translate(english))\nitem_categories['meta_category'] = item_categories['items_english'].apply(lambda x: x.split('-')[0])\nitem_categories['subtype'] = item_categories['items_english'].apply(lambda x: x.split('-')) \\\n                                                             .map(lambda x:x[1].strip() if len(x)>1 else x[0].strip())\nitem_categories.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2 Label encode these new features\n\n# Apply label encoding to item categories\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\nitem_categories[\"meta_category_code\"] = labelencoder.fit_transform(item_categories[\"meta_category\"])\nitem_categories[\"subtype_code\"] = labelencoder.fit_transform(item_categories[\"subtype\"])\nitem_categories = item_categories.drop([\"items_english\", \"meta_category\", \"subtype\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Translate the shop name and find towns\nshops[\"shops_english\"] = shops[\"shop_name\"].apply(lambda x: x.translate(english))\nshops[\"town\"] = shops[\"shops_english\"].apply(lambda x: x.split()[0])\nshops.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label encoding\nshops[\"town_code\"] = labelencoder.fit_transform(shops[\"town\"])\nshops = shops.drop([\"shops_english\", \"town\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make some useful features\n\n# Remove dates\n\nnewcols = train[\"date\"].str.split(\".\", expand=True)\nls = [\"day\", \"month\", \"year\"]\nfor i in range(len(ls)):\n    train.insert(i, ls[i], newcols[i])\ntrain['weekday'] = pd.to_datetime(train['date'], format = '%d.%m.%Y').dt.dayofweek\ntrain.pop(\"date\")\n\n# Add item category for each item\ntrain['item_category_id'] = [items['item_category_id'].values[i] for i in train['item_id'].values]\n\n# Add revenues\ntrain[\"revenue\"] = train[\"item_price\"]*train[\"item_cnt_day\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run this once \ntrain = train.merge(items, how='left')\ntrain = train.merge(item_categories, how='left')\ntrain = train.merge(shops, how='left')\n\ntrain.drop(\"item_name\",axis=1,inplace=True)\ntrain.drop(\"shop_name\",axis=1,inplace=True)\ntrain.drop(\"item_category_name\",axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2 Generate different combinations \n\nWe put 0 where there are no item/store combination to facilitate the process for the machine learning model "},{"metadata":{"trusted":true},"cell_type":"code","source":"#########\n# Create index cols \n#########\nindex_cols = ['shop_id', 'item_id', 'date_block_num']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#########\n# For every block num we generate unique combinations of shop_id and item_id \n#########\n\ngrid = list()\n\nfor block_num in train['date_block_num'].unique(): # 0-33\n    \"\"\"\n    For every block num we generate unique combinations of shop_id and item_id \n    \"\"\"\n    cur_shops = train.loc[train['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = train.loc[train['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])), dtype='int32'))# product will return cartesian product ex: # product('ABCD', 'xy') --> Ax Ay Bx By Cx Cy Dx Dy\n\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols, dtype=np.int32) # np.vstack will bring the array together ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# temp = []\n# for i in range(34):\n#     for shop in train_month['shop_id'].unique(): # shop uinique ids \n#         for item in train_month['item_id'].unique(): #item unique ids \n#             temp.append([i, shop, item]) # append to temp \n    \n# temp = pd.DataFrame(temp, columns=['date_block_num','shop_id','item_id']) # put temp on a dataframe \n\n# train_month = pd.merge(temp, train_month, on = ['date_block_num','shop_id','item_id'], how = 'left')\n# train_month.fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.head() # here we have created all possible combinations for date_block_num ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(grid) # here we have created all possible combinations for date_block_num ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What is the diferecen of grid and the previous table ?\n- Table 2 has len 10913850 while table 1 has 44486280\n- 2935849 len of train \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3 Mean encoding\nHere I added 9 new columns after item_category_id: \n- 'item_id_avg_item_price' 'item_id_sum_item_cnt_day' 'item_id_avg_item_cnt_day' 'shop_id_avg_item_price', 'shop_id_sum_item_cnt_day' \n'shop_id_avg_item_cnt_day' 'item_category_id_avg_item_price' 'item_category_id_sum_item_cnt_day' 'item_category_id_avg_item_cnt_day'"},{"metadata":{"trusted":true},"cell_type":"code","source":"#########\n# Mean encoding -- create diferent combinations \n#########\n\nmean_transactions = train.groupby(['date_block_num', 'shop_id', 'item_id']).agg({'item_cnt_day':'sum','item_price':'mean'}).reset_index() \n\n#After calculating the sum of the target variable and price_mean we merge it back to our grid and to items\nmean_transactions = pd.merge(grid,mean_transactions,on=['date_block_num', 'shop_id', 'item_id'],how='left').fillna(0)\nmean_transactions = pd.merge(mean_transactions, items, on='item_id',how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_transactions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(mean_transactions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#########\n# Mean encoding on difrent combinations to add 9 new feautures \n#########\n\n# Now we apply mean encodings: \nfor type_id in ['item_id', 'shop_id', 'item_category_id']:\n    for column_id, aggregator, aggtype in [('item_price',np.mean,'avg'),('item_cnt_day',np.sum,'sum'),('item_cnt_day',np.mean,'avg')]:\n        \n        mean_df = train.groupby([type_id,'date_block_num']).aggregate(aggregator).reset_index()[[column_id,type_id,'date_block_num']]\n        mean_df.columns = [type_id+'_'+aggtype+'_'+column_id,type_id,'date_block_num']\n        mean_transactions = pd.merge(mean_transactions, mean_df, on=['date_block_num',type_id], how='left')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(mean_transactions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'mean transactions shape: {mean_transactions.shape}')\nmean_transactions.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We still can see some outliers \nsns.boxplot(x ='item_cnt_day' , data = mean_transactions )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4 Generate lags"},{"metadata":{"trusted":true},"cell_type":"code","source":"#########\n# Generate lags \n#########\n\n# Create lags\nlag_variables  = list(mean_transactions.columns[7:]) + ['item_cnt_day']\nlags = [1, 2, 3, 6]\nfrom tqdm import tqdm_notebook\n\nfor lag in tqdm_notebook(lags): \n    sales_new_df = mean_transactions.copy()\n    sales_new_df.date_block_num += lag\n    sales_new_df = sales_new_df[['date_block_num','shop_id','item_id']+lag_variables]\n    sales_new_df.columns = ['date_block_num','shop_id','item_id']+ [lag_feat+'_lag_'+str(lag) for lag_feat in lag_variables]\n    mean_transactions = pd.merge(mean_transactions, sales_new_df,on=['date_block_num','shop_id','item_id'] ,how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check all the columns\nmean_transactions.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#########\n# Fill null values with zeros\n#########\n\nmean_transactions = mean_transactions[mean_transactions['date_block_num']>12] # from 13 to 33 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Null values were generated for lags \nmean_transactions.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sannity check \n\n# item_count = 0 \n# item_price = 0 \n# for feat in mean_transactions.columns:\n#     if 'item_cnt' in feat:\n#         item_count+= 1 \n#     elif 'item_price' in feat:\n#         item_price+= 1 \n\n# item_count + item_price ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#########\n#  Fill null values with 0 and median \n#########\n\nfor feat in mean_transactions.columns:\n    if 'item_cnt' in feat:\n        mean_transactions[feat]=mean_transactions[feat].fillna(0)\n    elif 'item_price' in feat:\n        mean_transactions[feat]=mean_transactions[feat].fillna(mean_transactions[feat].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#########\n# Drop the columns that wont be able to have prediction time. This columns are not lagged \n#########\n\ncols_to_drop = lag_variables[:-1] + ['item_price', 'item_name'] # dropping all target variables but not \"item_cnt_day\" cause is target\n#mean_transactions[cols_to_drop].nunique()\ntraining = mean_transactions.drop(cols_to_drop,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the dataset\ntraining.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling - Using XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"#########\n#  Define the X and target:\n#########\n\n# Create a matrix and define X and y train \n\n#Grab  everything within the columns that are not the target \nX_train = training.iloc[:, training.columns != 'item_cnt_day'].values \n\n#Grab  everything within the column target \ny_train = training.iloc[:, training.columns == 'item_cnt_day'].values\n\nxgbtrain = xgb.DMatrix(X_train , y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we should try to do a grid search to find optimal paramaters. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#########\n#  Define the hyperparamaters to pass to the model and train the model on these parameters (random paramaters)\n#########\n\nparam = {'max_depth':10, \n         'subsample':1,\n         'min_child_weight':0.5,\n         'eta':0.3, \n         'num_round':1000, \n         'seed':1,\n         'silent':0,\n         'eval_metric':'rmse'} # random parameters\n\n# Train the parameters on these metrics. Fit the model \n\nmodel1 = xgb.train(param, xgbtrain)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#########\n#  Plot the most important features \n#########\n\nx = xgb.plot_importance(model1)\nx.figure.set_size_inches(10, 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(training.columns)\ndel cols[cols.index('item_cnt_day')] # eliminate target feature col name\n[cols[x] for x in [2, 0, 5, 8, 4, 1, 3, 9, 33]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing the test dataset\n- We model the test set in the same way we modelled the train set. # So we need to manipulate the training set similarly to how we did in the first part of the notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"#########\n#  \n#########\n\n# here we give a value of 34 to all our block num  \ntest['date_block_num'] = 34 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge items to test set \ntest = pd.merge(test, items, on='item_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_transactions[['date_block_num','shop_id','item_id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cretae lags for test set \n#lags are lags = [1, 2, 3, 6]\nfor lag in tqdm_notebook(lags):\n\n    sales_new_df = mean_transactions.copy()\n    sales_new_df.date_block_num += lag\n    sales_new_df = sales_new_df[['date_block_num','shop_id','item_id']+lag_variables] # where did i compute this variables: lag_variables  = list(mean_transactions.columns[7:]) + ['item_cnt_day']\n    sales_new_df.columns = ['date_block_num','shop_id','item_id']+ [lag_feat+'_lag_'+str(lag) for lag_feat in lag_variables]\n    test = pd.merge(test, sales_new_df,on=['date_block_num','shop_id','item_id'] ,how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop columns I wont use and check that test matches with train columns \n_test = set(test.drop(['ID', 'item_name'], axis=1).columns)\n_training = set(training.drop('item_cnt_day',axis=1).columns)\nfor i in _test:\n    assert i in _training\nfor i in _training:\n    assert i in _test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert _training == _test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the id and item_name\ntest = test.drop(['ID', 'item_name'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feat in test.columns:\n    if 'item_cnt' in feat:\n        test[feat]=test[feat].fillna(0)\n    elif 'item_price' in feat:\n        test[feat]=test[feat].fillna(test[feat].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[['shop_id','item_id']+['item_cnt_day_lag_'+str(x) for x in [1,2,3]]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(training[training['shop_id'] == 5][training['item_id'] == 5037][training['date_block_num'] == 33]['item_cnt_day'])\nprint(training[training['shop_id'] == 5][training['item_id'] == 5037][training['date_block_num'] == 32]['item_cnt_day'])\nprint(training[training['shop_id'] == 5][training['item_id'] == 5037][training['date_block_num'] == 31]['item_cnt_day'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a xg matrix for test values \nxgbtest = xgb.DMatrix(test.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict on test set \n\npred1 = model1.predict(xgbtest) # the model is model, the model is already fitted = xgb.train(param, xgbtrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred1 = pred1.clip(0, 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_submission = pd.DataFrame({'ID':test.index,'item_cnt_month': pred1 })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I cannot validate with no target on the test dataset \n\nfinal_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To csv\n#final_submission.to_csv('submissions.csv',index=False )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Failed attempts"},{"metadata":{},"cell_type":"markdown","source":"Reason :The kernel crashes, due to the RAM "},{"metadata":{"trusted":true},"cell_type":"code","source":"# TEST\n#model2 = XGBRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#########\n# 11.1 Hyperparam optimization\n#########\n\n# params = {'max_depth':[3,5,7,9,12,15,17,25], \n#          'subsample':[0.6,0.7,0.8,0.9,1],\n#          'min_child_weight':[0.5,1,3,5,7],\n#          'eta':[0.01,0.015,0.025,0.05, 0.1, 0.3],\n#          'num_round':[1000], \n#          'seed':[1],\n#          'silent':[0],\n#          'eval_metric':['rmse']}\n\n\n# grid_search = GridSearchCV(estimator= model2 ,param_grid= params, scoring= 'neg_root_mean_squared_error',cv=5 , n_jobs = -1 )\n\n# grid_search.fit(X_train, y_train)\n\n# grid_search.best_params_ ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Solution 1 coursera"},{"metadata":{},"cell_type":"markdown","source":"- He tested multiple regression models (lasso and regression) and the proceeded to random forest regressor  "},{"metadata":{},"cell_type":"markdown","source":"## Solution 2 coursera"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}